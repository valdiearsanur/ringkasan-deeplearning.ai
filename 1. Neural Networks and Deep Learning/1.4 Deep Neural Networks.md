# Week 3 : Deep Neural Networks
Catatan untuk kursus **Neural Networks and Deep Learning**, kursus ke 1 (dari 5 kursus) Spesialisasi Deep Learning dari [Coursera](https://www.coursera.org/specializations/deep-learning) dan [DeepLearning.ai](http://deeplearning.ai/). Diajarkan oleh Andrew Ng.


## Table of contents

- [Week 3 : Deep Neural Networks](#week-3--deep-neural-networks)
  - [Table of contents](#table-of-contents)
  - [Deep Neural Networks](#deep-neural-networks)
    - [Deep L-layer neural network](#deep-l-layer-neural-network)
    - [Forward Propagation in a Deep Network](#forward-propagation-in-a-deep-network)
    - [Getting your matrix dimensions right](#getting-your-matrix-dimensions-right)
    - [Why deep representations?](#why-deep-representations)
    - [Building blocks of deep neural networks](#building-blocks-of-deep-neural-networks)
    - [Forward and Backward Propagation](#forward-and-backward-propagation)
    - [Parameters vs Hyperparameters](#parameters-vs-hyperparameters)
    - [What does this have to do with the brain](#what-does-this-have-to-do-with-the-brain)

## Deep Neural Networks

> Understand the key computations underlying deep learning, use them to build and train deep neural networks, and apply it to computer vision.

### Deep L-layer neural network

- Shallow Neural Net (NN) => NN dengan jumlah layer hingga 2.
- Deep NN => NN dengan jumlah layer 3 atau lebih
- Notasi : 
  - `L` = jumlah layer NN
  - `n[l]` = jumlah neuron pada layer `l`. `n[0]` menandakan jumlah neuron pada input layer, sedangkan `n[L]` menandakan jumlah neuron pada output layer.
  - `g[l]` = activation function.
  - `a[l] = g[l](z[l])`, `a[0] = x`, `a[l] = y'`
  - `w[l]` = weights yang digunakan pada `z[l]`
- These were the notation we will use for deep neural network.
- So we have:
  - A vector `n` of shape `(1, NoOfLayers+1)`
  - A vector `g` of shape `(1, NoOfLayers)`
  - A list of different shapes `w` based on the number of neurons on the previous and the current layer.
  - A list of different shapes `b` based on the number of neurons on the current layer.

### Forward Propagation in a Deep Network

- Forward propagation dengan satu buah input (notasi Z, A, B kecil):

  ```
  z[l] = W[l]a[l-1] + b[l]
  a[l] = g[l](a[l])
  ```

- Forward propagation dengan `m` buah input (notasi Z, A, B kapital):

  ```
  Z[l] = W[l]A[l-1] + B[l]
  A[l] = g[l](A[l])
  ```

- Komputasi Forward Propagation menggunakan for loop dari layer pertama hingga L

### Getting your matrix dimensions right

- Sebagian besar kesalahan pada komputasi NN ada pada dimensi yang tidak sesuai. Ada baiknya kita memastikan kesesuaian dimensi dari parameter, atau dapat melakukan debugging.
- Dimensi parameter `W` adalah `(n[l],n[l-1])` . Can be thought by right to left.
- Dimensi parameter `b` adalah `(n[l],1)`
- `dw` mempunyai dimensi (shape) yang sama dengan `W`, sedangkan `db` mempunyai dimensi yang sama dengan `b`
- Dimensi dari `Z[l],` `A[l]`, `dZ[l]`, dan `dA[l]` adalah `(n[l],m)`

### Why deep representations?

- Section ini membahas mengapa Deep NN lebih baik dari Shallow NN
- Deep NN membuat relasi data menjadi kompleks. In each layer it tries to make a relation with the previous layer. E.g.:
  - 1) Face recognition:
      - Image ==> Edges ==> Face parts ==> Faces ==> desired face
  - 2) Audio recognition:
      - Audio ==> Low level sound features like (sss,bb) ==> Phonemes ==> Words ==> Sentences
- Ilmuan Neural menyatakan bahwa Deep NN "berfikir" seperti otak manusia (simple ==> complex)
- Circuit theory and deep learning:
  - ![](Images/07.png)
- Sebaiknya saat memulai proyek jangan langsung menggunakan banyak Hidden Layer. Mulailah dari bentuk terkecil seperti Logistic Regression, lalu kembangkan menjadi Shallow NN dan seterusnya hingga Deep NN.

### Building blocks of deep neural networks

- Forward dan Back Propagation pada layer l:
  - ![Untitled](Images/10.png)
- Deep NN blocks:
  - ![](Images/08.png)

### Forward and Backward Propagation

- Pseudo code untuk Forward Propagation pada layer l:

  ```
  Input  A[l-1]
  Z[l] = W[l]A[l-1] + b[l]
  A[l] = g[l](Z[l])
  Output A[l], cache(Z[l])
  ```

- Pseudo code untuk Backward Propagation pada layer l:

  ```
  Input da[l], Caches
  dZ[l] = dA[l] * g'[l](Z[l])
  dW[l] = (dZ[l]A[l-1].T) / m
  db[l] = sum(dZ[l])/m                # Dont forget axis=1, keepdims=True
  dA[l-1] = w[l].T * dZ[l]            # The multiplication here are a dot product.
  Output dA[l-1], dW[l], db[l]
  ```

- If we have used our loss function then:

  ```
  dA[L] = (-(y/a) + ((1-y)/(1-a)))
  ```

### Parameters vs Hyperparameters

- Parameter NN adalah `W` and `b`
- Hyper parameters (parameter yang mengatur algoritma) adalah:
  - Learning rate.
  - Jumlah Iterasi.
  - Jumlah Hidden Layer `L`.
  - Jumlah Hidden Unit `n`.
  - Pilihan Activation Function (ReLU).
- You have to try values yourself of hyper parameters.
- Pada era awal dari DL and ML, Learning Rate disebut sebagai Parameter, namun sekarang ini disebut sebagai Hyperparameter.
- Pada kursus selanjutnya kita akan mempelajari mengenai optimasi Hyperparameter.

### What does this have to do with the brain

- The analogy that "It is like the brain" has become really an oversimplified explanation.
- There is a very simplistic analogy between a single logistic unit and a single neuron in the brain.
- No human today understand how a human brain neuron works.
- No human today know exactly how many neurons on the brain.
- Deep learning in Andrew's opinion is very good at learning very flexible, complex functions to learn X to Y mappings, to learn input-output mappings (supervised learning).
- The field of computer vision has taken a bit more inspiration from the human brains then other disciplines that also apply deep learning.
- NN is a small representation of how brain work. The most near model of human brain is in the computer vision (CNN)


<br><br>
Sumber :
- Deeplearning.ai course
- https://github.com/mbadry1/DeepLearning.ai-Summary
- https://github.com/ppant/deeplearning.ai-notes

Catatan dari by [Muhammad Valdie Arsanur](mailto:mvasecondary@gmail.com)
